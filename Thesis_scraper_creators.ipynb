{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube creator page scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Basic steps\n",
    "In the first part we will start by importing the right packages and load the data required to let the scraper work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "First we import the packages required for the scraper to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import selenium.webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 \n",
    "First we import the dataset with the Youtubers name or Code which will be used to make the scraper work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>channelId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UCIv0wH_CdWgttwX6B-Cyt8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>UCEU5ZK7DwN9ppqPFJiGah3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>UC8CX0LD98EDXl4UYX1MDCXg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>UCF_fDSgPpBQuh1MsUTgIARQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>UCUtmzHuW43bFbUP-Xl0TTKA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 channelId\n",
       "0           1  UCIv0wH_CdWgttwX6B-Cyt8w\n",
       "1           2  UCEU5ZK7DwN9ppqPFJiGah3A\n",
       "2           3  UC8CX0LD98EDXl4UYX1MDCXg\n",
       "3           4  UCF_fDSgPpBQuh1MsUTgIARQ\n",
       "4           5  UCUtmzHuW43bFbUP-Xl0TTKA"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_thesis = pd.read_csv(r'creators_test.csv', low_memory = False, sep = \",\")\n",
    "dataset_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    UCIv0wH_CdWgttwX6B-Cyt8w\n",
       "1    UCEU5ZK7DwN9ppqPFJiGah3A\n",
       "2    UC8CX0LD98EDXl4UYX1MDCXg\n",
       "3    UCF_fDSgPpBQuh1MsUTgIARQ\n",
       "4    UCUtmzHuW43bFbUP-Xl0TTKA\n",
       "Name: channelId, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_code = dataset_thesis['channelId']\n",
    "dataset_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Scraper videos page\n",
    "This scraper scrapes the information from the \"video's\" page which contains the information of the account name, number of subscribers and the number of video's posted on that account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "In the first extracting step we extract the Youtube pages which can be accessed by adding **'channel'** after the regulare \"youtube.com\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-986cfe0c8f8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0myoutube_channel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0myoutube_channel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_youtube_channel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0myoutube_channel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_code' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_youtube_channel(codes):\n",
    "    youtube_channel = [] \n",
    "    for code in tqdm(codes):\n",
    "        driver = selenium.webdriver.Chrome()\n",
    "        driver.get(\"https://www.youtube.com/channel/\" + code + \"/videos\")\n",
    "    \n",
    "        time.sleep(5)\n",
    "        \n",
    "        #Accepting the cookies\n",
    "        accept = driver.find_elements_by_class_name(\"VfPpkd-dgl2Hf-ppHlrf-sM5MNb\")[5]\n",
    "        accept.click()\n",
    "    \n",
    "        while True:\n",
    "            scroll_height = 5000\n",
    "            document_height_before = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            driver.execute_script(f\"window.scrollTo(0, {document_height_before + scroll_height});\")\n",
    "            time.sleep(1.5)\n",
    "            document_height_after = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if document_height_after == document_height_before:\n",
    "                break\n",
    "              \n",
    "        #for page_url in page_urls:\n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "          \n",
    "        def num_video():\n",
    "            try:\n",
    "                num_video = len(soup.find_all(class_=\"yt-simple-endpoint style-scope ytd-grid-video-renderer\"))\n",
    "                return(num_video)\n",
    "            except:\n",
    "                return  \n",
    "       \n",
    "    \n",
    "        def channel_id():\n",
    "            try:\n",
    "                channel_id = soup.find(\"meta\", itemprop=\"channelId\")[\"content\"]\n",
    "                return(channel_id)\n",
    "            except:\n",
    "                return \n",
    "    \n",
    "        youtube_channel.append({\"num_video\" : num_video(),\n",
    "                               \"channel_id\" : channel_id()})                         \n",
    "            \n",
    "        sleep (1)\n",
    "            \n",
    "    return youtube_channel\n",
    "\n",
    "youtube_channel = extract_youtube_channel(dataset_code)\n",
    "youtube_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_channel_dataframe_thesis_p1 = pd.DataFrame(youtube_channel)\n",
    "youtube_channel_dataframe_thesis_p1.to_csv(\"creator_characteristics_p1.csv\", index=False)\n",
    "youtube_channel_dataframe_thesis_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first scraper we extracted the number of videos, the second scraper will be used to detect the number of total views and since when somebody is member of YouTube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Scraper about page\n",
    "This scraper scrapes the information from the \"about\" page which contains the information of the account name, number of subscribers, member since and the total number of views an account achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "In the second extracting step from Part 3 we extract the Youtube pages which can be accessed by adding a **'channel'** after the regulare \"youtube.com\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_youtube_channel_p2(codes):\n",
    "    youtube_channel_p2 = [] \n",
    "    for code in tqdm(codes):\n",
    "        driver = selenium.webdriver.Chrome()\n",
    "        driver.get(\"https://www.youtube.com/channel/\" + code + \"/about\")\n",
    "    \n",
    "        time.sleep(5)\n",
    "        \n",
    "        #Accepting the cookies\n",
    "        accept = driver.find_elements_by_class_name(\"VfPpkd-dgl2Hf-ppHlrf-sM5MNb\")[5]\n",
    "        accept.click()\n",
    "               \n",
    "        #for page_url in page_urls:\n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "        def name():\n",
    "            try:\n",
    "                name = soup.find(attrs={\"id\": \"text-container\"}).text\n",
    "                name = name.replace(\"\\n\", \"\")\n",
    "                return(name)\n",
    "            except:\n",
    "                return \n",
    "            \n",
    "        def subscribe():\n",
    "            try:\n",
    "                subscribe = soup.find(attrs={\"id\": \"subscriber-count\"}).text\n",
    "                subscribe = subscribe.replace(\"\\xa0\", \"\")\n",
    "                subscribe = subscribe.replace(\" abonnees\", \"\")\n",
    "                return(subscribe)\n",
    "            except:\n",
    "                return \n",
    "            \n",
    "        def views():\n",
    "            try:\n",
    "                views = soup.find(attrs={\"id\": \"right-column\"}).get_text()\n",
    "                views = views.split('\\n')[3]\n",
    "                views = views.replace(\" weergaven\", \"\")\n",
    "                return(views)\n",
    "            except:\n",
    "                return \n",
    "    \n",
    "        def member_since():\n",
    "            try:\n",
    "                member_since = soup.find(attrs={\"id\": \"right-column\"}).get_text()\n",
    "                member_since = member_since.split('\\n')[2]\n",
    "                member_since = member_since.replace(\"Lid geworden op\", \"\")\n",
    "                return(member_since)\n",
    "            except:\n",
    "                return  \n",
    "        \n",
    "        def channel_id():\n",
    "            try:\n",
    "                channel_id = soup.find(\"meta\", itemprop=\"channelId\")[\"content\"]\n",
    "                return(channel_id)\n",
    "            except:\n",
    "                return \n",
    "        \n",
    "        youtube_channel_p2.append({\"creator\": name(),\n",
    "                                    \"subscribers\" : subscribe(),\n",
    "                                    \"views\" : views(),\n",
    "                                    \"member_since\" : member_since(),\n",
    "                                    \"channel_id\" : channel_id()})                       \n",
    "            \n",
    "        sleep (1)\n",
    "            \n",
    "    return youtube_channel_p2\n",
    "\n",
    "youtube_channel_p2 = extract_youtube_channel_p2(dataset_code)\n",
    "youtube_channel_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_channel_p2_dataframe_thesis = pd.DataFrame(youtube_channel_p2)\n",
    "youtube_channel_p2_dataframe_thesis.to_csv(\"creator_characteristics_part_channel_p2.csv\", index=False)\n",
    "youtube_channel_p2_dataframe_thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Merge\n",
    "In this last part of the scraper we will merge the two datasets we have created earlier on together to one complete dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataframe_thesis = youtube_channel_dataframe_thesis_p1.merge(youtube_channel_p2_dataframe_thesis, how=\"inner\", on =[\"channel_id\"])\n",
    "complete_dataframe_thesis.to_csv(\"creator_characteristics_complete.csv\", index=False)\n",
    "complete_dataframe_thesis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
